{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem Description\n",
    "\n",
    "The k-armed bandit problem is a simple reinforcement learning problem where the environment contains only a single state. The agent is presented with k different actions that it can take, and each action gives out a reward generated from some stationary probability distribution. The agent starts out not knowing which action is the best action, so it must first explore all of the possibilities to figure out the expected reward of each action.\n",
    "\n",
    "The real-life analogy of this problem is a gambler (bandit) that wants to maximize their money (reward) by playing the slot machines, but there are k slot machines, each with its own probabilistic payoff. The gambler must then figure out which slot machine is the most profitable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from functools import partial\n",
    "\n",
    "def reset_seed(seed=42):\n",
    "    np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Environment\n",
    "\n",
    "There will be two kinds of environments: discrete and continuous. A discrete bandit environment just means that there are some finite number of payoffs, each with its own probability (total probability must add up to one). A continuous bandit environment generates payoffs based on some probability distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscreteBandit():\n",
    "    \"\"\"\n",
    "    A discrete bandit environment.\n",
    "    \n",
    "    Args:\n",
    "        payoffs: (ndarray of shape [n_arms, n_payoffs]) the payoffs for each action\n",
    "        probabilities (ndarray of shape [n_arms, n_payoffs]) the probability for every payoff of every action\n",
    "    \"\"\"\n",
    "    def __init__(self, payoffs, probabilities):\n",
    "        self.payoffs = payoffs\n",
    "        self.probabilities = probabilities\n",
    "        \n",
    "    def step(self, x):\n",
    "        return np.random.choice(self.payoffs[x], p=self.probabilities[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reward: -111\n"
     ]
    }
   ],
   "source": [
    "reset_seed(42)\n",
    "\n",
    "payoffs = np.array([[-1, 0, 1],\n",
    "                    [-2, 0, 5],\n",
    "                    [0, 4, 8],\n",
    "                    [-2, -1, 1]])\n",
    "\n",
    "probabilities = np.array([[0.45, 0.1, 0.45],\n",
    "                          [0.45, 0.1, 0.45],\n",
    "                          [0.98, 0.01, 0.01],\n",
    "                          [0.98, 0.01, 0.01]])\n",
    "\n",
    "env = DiscreteBandit(payoffs, probabilities)\n",
    "\n",
    "max_steps = 1000\n",
    "total_reward = 0\n",
    "\n",
    "for step in range(max_steps):\n",
    "    total_reward += env.step(np.random.choice(len(payoffs)))  # choose a random action\n",
    "    \n",
    "print(\"Total reward:\", total_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContinuousBandit():\n",
    "    \"\"\"\n",
    "    A continuous bandit environment.\n",
    "    \n",
    "    Args:        \n",
    "    \"\"\"\n",
    "    def __init__(self, reward_distributions):\n",
    "        self.reward_distributions = reward_distributions\n",
    "        \n",
    "    def step(self, x):\n",
    "        return self.reward_distributions[x]()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: -2.51 \t Sigma: 0.78\n",
      "Mean: 9.01 \t Sigma: 0.29\n",
      "Mean: 4.64 \t Sigma: 4.33\n",
      "Mean: 1.97 \t Sigma: 3.01\n",
      "Mean: -6.88 \t Sigma: 3.54\n",
      "Total reward: 848.69\n"
     ]
    }
   ],
   "source": [
    "reset_seed(42)\n",
    "\n",
    "mu = [np.random.uniform(-10, 10) for _ in range(5)]\n",
    "sigma = [np.random.uniform(0, 5) for _ in range(5)]\n",
    "reward_distributions = [partial(np.random.normal, m, s) for m, s in zip(mu, sigma)]\n",
    "\n",
    "for m, s in zip(mu, sigma):\n",
    "    print(\"Mean: %.2f \\t Sigma: %.2f\" % (m, s))\n",
    "\n",
    "env = ContinuousBandit(reward_distributions)\n",
    "\n",
    "max_steps = 1000\n",
    "total_reward = 0\n",
    "\n",
    "for step in range(max_steps):\n",
    "    total_reward += env.step(np.random.choice(len(reward_distributions)))  # choose a random action\n",
    "    \n",
    "print(\"Total reward: %.2f\" % total_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
