{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem Description\n",
    "\n",
    "The k-armed bandit problem is a simple reinforcement learning problem where the environment contains only a single state. The agent is presented with k different actions that it can take, and each action gives out a reward generated from some stationary probability distribution. The agent starts out not knowing which action is the best action, so it must first explore all of the possibilities to figure out the expected reward of each action.\n",
    "\n",
    "The real-life analogy of this problem is a gambler (bandit) that wants to maximize their money (reward) by playing the slot machines, but there are k slot machines, each with its own probabilistic payoff. The gambler must then figure out which slot machine is the most profitable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from functools import partial\n",
    "\n",
    "def reset_seed(seed=42):\n",
    "    np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Environment\n",
    "\n",
    "There will be two kinds of environments: discrete and continuous. A discrete bandit environment just means that there are some finite number of payoffs, each with its own probability (total probability must add up to one). A continuous bandit environment generates payoffs based on some probability distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscreteBandit():\n",
    "    \"\"\"\n",
    "    A discrete bandit environment.\n",
    "    \n",
    "    Args:\n",
    "        payoffs: (ndarray of shape [n_arms, n_payoffs]) the payoffs for each action\n",
    "        probabilities (ndarray of shape [n_arms, n_payoffs]) the probability for every payoff of every action\n",
    "    \"\"\"\n",
    "    def __init__(self, payoffs, probabilities):\n",
    "        self.payoffs = payoffs\n",
    "        self.probabilities = probabilities\n",
    "        \n",
    "    def step(self, x):\n",
    "        return np.random.choice(self.payoffs[x], p=self.probabilities[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reward: -111\n"
     ]
    }
   ],
   "source": [
    "reset_seed(42)\n",
    "\n",
    "payoffs = np.array([[-1, 0, 1],\n",
    "                    [-2, 0, 5],\n",
    "                    [0, 4, 8],\n",
    "                    [-2, -1, 1]])\n",
    "\n",
    "probabilities = np.array([[0.45, 0.1, 0.45],\n",
    "                          [0.45, 0.1, 0.45],\n",
    "                          [0.98, 0.01, 0.01],\n",
    "                          [0.98, 0.01, 0.01]])\n",
    "\n",
    "env = DiscreteBandit(payoffs, probabilities)\n",
    "\n",
    "max_steps = 1000\n",
    "total_reward = 0\n",
    "\n",
    "for step in range(max_steps):\n",
    "    total_reward += env.step(np.random.choice(len(payoffs)))  # choose a random action\n",
    "    \n",
    "print(\"Total reward:\", total_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContinuousBandit():\n",
    "    \"\"\"\n",
    "    A continuous bandit environment.\n",
    "    \n",
    "    Args:        \n",
    "    \"\"\"\n",
    "    def __init__(self, reward_distributions):\n",
    "        self.reward_distributions = reward_distributions\n",
    "        \n",
    "    def step(self, x):\n",
    "        return self.reward_distributions[x]()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: -2.51 \t Sigma: 0.78\n",
      "Mean: 9.01 \t Sigma: 0.29\n",
      "Mean: 4.64 \t Sigma: 4.33\n",
      "Mean: 1.97 \t Sigma: 3.01\n",
      "Mean: -6.88 \t Sigma: 3.54\n",
      "Total reward: 848.69\n"
     ]
    }
   ],
   "source": [
    "reset_seed(42)\n",
    "\n",
    "mu = [np.random.uniform(-10, 10) for _ in range(5)]\n",
    "sigma = [np.random.uniform(0, 5) for _ in range(5)]\n",
    "reward_distributions = [partial(np.random.normal, m, s) for m, s in zip(mu, sigma)]\n",
    "\n",
    "for m, s in zip(mu, sigma):\n",
    "    print(\"Mean: %.2f \\t Sigma: %.2f\" % (m, s))\n",
    "\n",
    "env = ContinuousBandit(reward_distributions)\n",
    "\n",
    "max_steps = 1000\n",
    "total_reward = 0\n",
    "\n",
    "for step in range(max_steps):\n",
    "    total_reward += env.step(np.random.choice(len(reward_distributions)))  # choose a random action\n",
    "    \n",
    "print(\"Total reward: %.2f\" % total_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Greedy Policy\n",
    "\n",
    "Because we are interested in creating an artificial intelligence, our agent needs to do better than just taking a random action at each step. The agent needs to be able to make decisions based on something. In RL lingo, we call this decision-making ability a \"policy\". The simplest non-random policy is probably the greedy policy, where the agent chooses what it thinks as the optimal action at each given step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reward: 9008.13\n"
     ]
    }
   ],
   "source": [
    "reset_seed(42)\n",
    "\n",
    "n = 5\n",
    "\n",
    "mu = [np.random.uniform(-10, 10) for _ in range(n)]\n",
    "sigma = [np.random.uniform(0, 5) for _ in range(n)]\n",
    "reward_distributions = [partial(np.random.normal, m, s) for m, s in zip(mu, sigma)]\n",
    "\n",
    "# Store rewards obtained at each step. This acts as a \"memory\" and can be used to make decisions.\n",
    "rewards = [[] for _ in range(n)]\n",
    "total_rewards = [0 for _ in range(n)]\n",
    "n_tries = [0 for _ in range(n)]\n",
    "\n",
    "def expected_reward(a):\n",
    "    if n_tries[a] == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return total_rewards[a] / n_tries[a]\n",
    "    \n",
    "def greedy_policy():\n",
    "    return np.argmax([expected_reward(a) for a in range(n)])\n",
    "\n",
    "env = ContinuousBandit(reward_distributions)\n",
    "\n",
    "max_steps = 1000\n",
    "\n",
    "for step in range(max_steps):\n",
    "    a = greedy_policy()\n",
    "    reward = env.step(a)\n",
    "    rewards[a].append(reward)\n",
    "    total_rewards[a] += reward\n",
    "    n_tries[a] += 1\n",
    "    \n",
    "print(\"Total reward: %.2f\" % sum(total_rewards))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T\n",
    "\n",
    "The greedy policy has an obvious weakness, namely it is short-sighted. A typical reinforcement learning environment is uncertain, the agent almost always starts out not knowing which action is the best and what is the expected reward of each action. It faces the dilemma of choosing between what it thinks is the optimal action (exploitation), and choosing some other action in the hope of improving its knowledge so that it can make better decisions in the future (exploration). This is called the exploration-exploitation tradeoff.\n",
    "\n",
    "In the greedy example above, the agent performs no exploration at all, it always chooses what it thinks as the best action at each step. As a result, it stops looking when it finds an action that yields a positive reward (on average), and doesn't consider the possibility that there might be some other action that gives better rewards. One way to integrate exploration into the greedy policy is to perform each action some number of times to gather data before going greedy all the way until the end. Since there are 1000 time steps, one option is to perform each action 20 times for the first 100 time steps, then choose the best action for the other 900. But this approach seems arbitrary, and it doesn't guarantee convergence (there's a non-zero chance that 20 data points per action isn't enough to determine the true expected reward).\n",
    "\n",
    "One exploration policy that has been around since the dawn of RL is the epsilon-greedy policy. It works by setting some exploration coefficient, epsilon, that determines the probability of choosing a random action instead of taking the greedy action. So, if epsilon is set to 0.1, then at any given time step the agent has 10% probability of choosing a random action and 90% probability of choosing the greedy action. This policy guarantees convergence, each action will be taken an infinite number of times as time goes to infinity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reward: 8036.72\n"
     ]
    }
   ],
   "source": [
    "reset_seed(42)\n",
    "\n",
    "n = 5\n",
    "\n",
    "mu = [np.random.uniform(-10, 10) for _ in range(n)]\n",
    "sigma = [np.random.uniform(0, 5) for _ in range(n)]\n",
    "reward_distributions = [partial(np.random.normal, m, s) for m, s in zip(mu, sigma)]\n",
    "\n",
    "# Store rewards obtained at each step. This acts as a \"memory\" and can be used to make decisions.\n",
    "rewards = [[] for _ in range(n)]\n",
    "total_rewards = [0 for _ in range(n)]\n",
    "n_tries = [0 for _ in range(n)]\n",
    "\n",
    "def expected_reward(a):\n",
    "    if n_tries[a] == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return total_rewards[a] / n_tries[a]\n",
    "    \n",
    "def epsilon_greedy(epsilon):\n",
    "    if np.random.rand() < epsilon:\n",
    "        return np.random.randint(n)  # random action\n",
    "    else:\n",
    "        return np.argmax([expected_reward(a) for a in range(n)])  # greedy action\n",
    "\n",
    "env = ContinuousBandit(reward_distributions)\n",
    "\n",
    "max_steps = 1000\n",
    "epsilon = 0.1\n",
    "\n",
    "for step in range(max_steps):\n",
    "    a = epsilon_greedy(epsilon)\n",
    "    reward = env.step(a)\n",
    "    rewards[a].append(reward)\n",
    "    total_rewards[a] += reward\n",
    "    n_tries[a] += 1\n",
    "    \n",
    "print(\"Total reward: %.2f\" % sum(total_rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
